{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "Below is a toy example of loading encoded CTR dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['123 gender_1 age_group_0', '456 gender_0 age_group_1', '789 gender_1 age_group_0']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "def row_to_string(row):\n",
    "    user_id_str = str(row['user_id'])\n",
    "    other_columns_str = ' '.join([f\"{col}_{row[col]}\" for col in row.index if col != 'user_id'])\n",
    "    return f\"{user_id_str} {other_columns_str}\"\n",
    "\n",
    "def dataframe_to_string(df):\n",
    "    return df.apply(row_to_string, axis=1)\n",
    "\n",
    "# Toy data\n",
    "data = {\n",
    "    'user_id': [123, 456, 789],\n",
    "    'gender': [1, 0, 1],\n",
    "    'age_group': [0, 1, 0]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "string_representation = dataframe_to_string(df)\n",
    "print(string_representation.tolist())\n",
    "\n",
    "data_dict = {\n",
    "    'row_string': string_representation\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'row_string': 0    123 gender_1 age_group_0\n",
       " 1    456 gender_0 age_group_1\n",
       " 2    789 gender_1 age_group_0\n",
       " dtype: object}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Custom Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['1', '2', '3', 'gender_1', 'age_group_0']\n",
      "Token IDs: [1, 2, 3, 11, 13]\n"
     ]
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "class CustomTokenizer(PreTrainedTokenizer):\n",
    "    def __init__(self, vocab, **kwargs):        \n",
    "        self.vocab = vocab\n",
    "        self.ids_to_tokens = {i: token for i, token in enumerate(self.vocab)}\n",
    "        self.tokens_to_ids = {token: i for i, token in enumerate(self.vocab)}\n",
    "        \n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        tokens = []\n",
    "        for part in text.split():\n",
    "            if part.isdigit():\n",
    "                # Split digits into separate tokens\n",
    "                tokens.extend(list(part))\n",
    "            else:\n",
    "                tokens.append(part)\n",
    "        return tokens\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return [self.tokens_to_ids[token] for token in tokens]\n",
    "\n",
    "    def convert_ids_to_tokens(self, ids):\n",
    "        return [self.ids_to_tokens[_id] for _id in ids]\n",
    "\n",
    "    def _convert_token_to_id(self, token):\n",
    "        return self.tokens_to_ids.get(token, self.tokens_to_ids.get('[UNK]'))\n",
    "\n",
    "    def _convert_id_to_token(self, index):\n",
    "        return self.ids_to_tokens.get(index, '[UNK]')\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return self.tokens_to_ids\n",
    "\n",
    "# Toy data vocabulary\n",
    "# Replace with actual CTR data levels.\n",
    "vocab = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \n",
    "         \"gender_0\", \"gender_1\", \"gender_2\", \n",
    "         \"age_group_0\", \"age_group_1\"]\n",
    "\n",
    "tokenizer = CustomTokenizer(vocab=vocab)\n",
    "\n",
    "row_string = string_representation[0]\n",
    "tokens = tokenizer.tokenize(row_string)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Token IDs: {token_ids}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47ca867a10a44ce48ceffffbee1cd466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'row_string': '123 gender_1 age_group_0', 'input_ids': [1, 2, 3, 11, 13], 'labels': [1, 2, 3, 11, 13]}\n"
     ]
    }
   ],
   "source": [
    "# Define tokenizer based on the vocab\n",
    "tokenizer = CustomTokenizer(vocab=vocab)\n",
    "\n",
    "# Tokenizing the dataset\n",
    "def tokenize_example(example):\n",
    "    tokens = tokenizer.tokenize(example['row_string'])\n",
    "    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "     # Assign labels same as input_ids for causal LM training\n",
    "    return {\"input_ids\": token_ids, \"labels\": token_ids} \n",
    "\n",
    "# Apply the tokenizer to the dataset\n",
    "tokenized_dataset = dataset.map(tokenize_example)\n",
    "\n",
    "# Show an example from the tokenized dataset\n",
    "print(tokenized_dataset[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 00:03, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.676800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.133800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=20, training_loss=0.9052952706813813, metrics={'train_runtime': 3.8237, 'train_samples_per_second': 7.846, 'train_steps_per_second': 5.23, 'total_flos': 76550400000.0, 'train_loss': 0.9052952706813813, 'epoch': 10.0})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the model\n",
    "def load_model(from_scratch=True):\n",
    "    if from_scratch:\n",
    "        model = GPT2LMHeadModel(config=GPT2LMHeadModel.config_class(vocab_size=len(vocab)))\n",
    "    else:\n",
    "        model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "        model.resize_token_embeddings(len(vocab))  # Resize embeddings if vocab size differs\n",
    "    return model\n",
    "\n",
    "model = load_model(from_scratch=True)  # Set to False to use pre-trained model\n",
    "save_dir = \"./results_custom_tokenizer\"\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=save_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=10,\n",
    "    save_total_limit=2,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_test_dataset = tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate text from the model\n",
    "def generate_text_from_prompt(model, tokenizer, input_ids, max_length=5, prompt_length=3):\n",
    "    prompt = input_ids[:prompt_length]\n",
    "    input_ids_tensor = torch.tensor([prompt], dtype=torch.long).to(model.device)\n",
    "\n",
    "    # Generate the completion\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids=input_ids_tensor,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=1,\n",
    "            pad_token_id=tokenizer.tokens_to_ids.get('[PAD]', None),\n",
    "            eos_token_id=tokenizer.tokens_to_ids.get('[EOS]', None)\n",
    "        )\n",
    "    \n",
    "    # Convert the output IDs to tokens and then to string\n",
    "    generated_sequence = output[0].tolist()\n",
    "    generated_text = tokenizer.convert_ids_to_tokens(generated_sequence)\n",
    "    return \" \".join(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original row string: 123 gender_1 age_group_0\n",
      "Generated completion: 1 2 3 gender_1 age_group_0\n",
      "\n",
      "Original row string: 456 gender_0 age_group_1\n",
      "Generated completion: 4 5 6 gender_0 age_group_1\n",
      "\n",
      "Original row string: 789 gender_1 age_group_0\n",
      "Generated completion: 7 8 9 gender_1 age_group_0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, example in enumerate(tokenized_test_dataset):\n",
    "    print(f\"Original row string: {example['row_string']}\")\n",
    "    \n",
    "    # Use the first half of the input_ids as prompt\n",
    "    generated_text = generate_text_from_prompt(model, tokenizer, example['input_ids'])\n",
    "    \n",
    "    # Print the generated text\n",
    "    print(f\"Generated completion: {generated_text}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
