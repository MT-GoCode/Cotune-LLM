{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/cotune/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Data loading\n",
    "import pandas as pd\n",
    "from datasets import Dataset, load_from_disk\n",
    "\n",
    "# Tokenizer Definition\n",
    "import json\n",
    "from transformers import PreTrainedTokenizer\n",
    "\n",
    "# Model training / generation\n",
    "from transformers import GPT2Config, GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "Load dataset created during encoding stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_file_path = \"encoded_dataset\" # Dataset created by text encoding\n",
    "dataset = load_from_disk(dataset_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '137787 ad_click_list_v001_28621 ad_click_list_v001_21424 ad_click_list_v001_24055 ad_click_list_v001_17305 ad_click_list_v001_31470 ad_click_list_v002_1203 ad_click_list_v002_1172 ad_click_list_v002_1112 ad_click_list_v002_1775 ad_click_list_v002_1041 ad_click_list_v003_162 ad_click_list_v003_240 ad_click_list_v003_343 ad_click_list_v003_246 ad_click_list_v003_312 ad_close_list_v001_24107 ad_close_list_v002_1218 ad_close_list_v003_173 hispace_app_tags_43 u_newsCatInterests_140 u_newsCatInterests_112 u_newsCatInterests_16 u_newsCatInterests_176 u_newsCatInterests_207 u_newsCatDislike_0 u_click_ca2_news_112 u_click_ca2_news_168 u_click_ca2_news_140 u_click_ca2_news_207 u_click_ca2_news_15 i_entities_5b212d9859cc262a2d9f4731b8e1890be315e4d27e4d4602bdc993ec955cdfac i_entities_8e1358ee2230f9112e0464bba2cc119224a6849fd6477d6a316eb358e0bbff14 i_entities_064d7e92c0b22a54f65e6193db3f201ed58258a1f17bed583f1359423fcf7331 i_entities_c81ec0fd7307cf51be43e50261cf60c724d1972d358be6ddb8a1f1cb191adf98 i_entities_9a7ddbfecfcdb8eb62f864021b755cff68ff16d8e9d644aa3b57f876fd7b4538 u_newsCatInterestsST_x_140 u_newsCatInterestsST_x_207 u_newsCatInterestsST_x_176 u_newsCatInterestsST_x_157 u_newsCatInterestsST_x_52 u_newsCatInterestsST_y_140 u_newsCatInterestsST_y_157 u_newsCatInterestsST_y_15 u_newsCatInterestsST_y_50 u_newsCatInterestsST_y_168 adv_id_17944 adv_prim_id_1779 age_3 app_score_10.0 app_second_class_18 cillabel_-1 city_103 city_rank_2 creat_type_cd_8 device_name_211 device_size_2397 e_ch_19 e_et_202206100923 e_m_1363 e_pl_90 e_po_9 e_rn_1 e_section_0 emui_dev_13 gender_2 i_cat_119 i_dislikeTimes_5 i_docId_6913d30d88bf7b19268d09f735508ef6a3d0517e i_dtype_12 i_regionEntity_2836 i_s_sourceId_c07287c465dc24013ec0d11a16ec6b4fb88658be i_upTimes_6 inter_type_cd_4 label_-1 log_id_762506 net_type_7 pro_0 pt_d_202206100242 residence_13 series_dev_30 series_group_3 site_id_1 slot_id_59 spread_app_id_312 task_id_15589 u_browserLifeCycle_17 u_browserMode_14 u_feedLifeCycle_x_17 u_feedLifeCycle_y_17 u_phonePrice_15 u_refreshTimes_x_8 u_refreshTimes_y_8 u_userId_137787 user_id_137787'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset['test'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON file containing the unique token vocabulary\n",
    "with open('vocab_map.json', 'r') as f:\n",
    "    token_vocab = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Custom Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens with BOS/EOS and number handling: ['[BOS]', '1', '3', '7', '7', '8', '7', 'ad_click_list_v001_28621', 'ad_click_list_v001_21424', 'ad_click_list_v001_24055', 'ad_click_list_v001_17305', 'ad_click_list_v001_31470', 'ad_click_list_v002_1203', 'ad_click_list_v002_1172', 'ad_click_list_v002_1112', 'ad_click_list_v002_1775', 'ad_click_list_v002_1041', 'ad_click_list_v003_162', 'ad_click_list_v003_240', 'ad_click_list_v003_343', 'ad_click_list_v003_246', 'ad_click_list_v003_312', 'ad_close_list_v001_24107', 'ad_close_list_v002_1218', 'ad_close_list_v003_173', 'hispace_app_tags_43', 'u_newsCatInterests_140', 'u_newsCatInterests_112', 'u_newsCatInterests_16', 'u_newsCatInterests_176', 'u_newsCatInterests_207', 'u_newsCatDislike_0', 'u_click_ca2_news_112', 'u_click_ca2_news_168', 'u_click_ca2_news_140', 'u_click_ca2_news_207', 'u_click_ca2_news_15', 'i_entities_5b212d9859cc262a2d9f4731b8e1890be315e4d27e4d4602bdc993ec955cdfac', 'i_entities_8e1358ee2230f9112e0464bba2cc119224a6849fd6477d6a316eb358e0bbff14', 'i_entities_064d7e92c0b22a54f65e6193db3f201ed58258a1f17bed583f1359423fcf7331', 'i_entities_c81ec0fd7307cf51be43e50261cf60c724d1972d358be6ddb8a1f1cb191adf98', 'i_entities_9a7ddbfecfcdb8eb62f864021b755cff68ff16d8e9d644aa3b57f876fd7b4538', 'u_newsCatInterestsST_x_140', 'u_newsCatInterestsST_x_207', 'u_newsCatInterestsST_x_176', 'u_newsCatInterestsST_x_157', 'u_newsCatInterestsST_x_52', 'u_newsCatInterestsST_y_140', 'u_newsCatInterestsST_y_157', 'u_newsCatInterestsST_y_15', 'u_newsCatInterestsST_y_50', 'u_newsCatInterestsST_y_168', 'adv_id_17944', 'adv_prim_id_1779', 'age_3', 'app_score_10.0', 'app_second_class_18', 'cillabel_-1', 'city_103', 'city_rank_2', 'creat_type_cd_8', 'device_name_211', 'device_size_2397', 'e_ch_19', 'e_et_202206100923', 'e_m_1363', 'e_pl_90', 'e_po_9', 'e_rn_1', 'e_section_0', 'emui_dev_13', 'gender_2', 'i_cat_119', 'i_dislikeTimes_5', 'i_docId_6913d30d88bf7b19268d09f735508ef6a3d0517e', 'i_dtype_12', 'i_regionEntity_2836', 'i_s_sourceId_c07287c465dc24013ec0d11a16ec6b4fb88658be', 'i_upTimes_6', 'inter_type_cd_4', 'label_-1', 'log_id_762506', 'net_type_7', 'pro_0', 'pt_d_202206100242', 'residence_13', 'series_dev_30', 'series_group_3', 'site_id_1', 'slot_id_59', 'spread_app_id_312', 'task_id_15589', 'u_browserLifeCycle_17', 'u_browserMode_14', 'u_feedLifeCycle_x_17', 'u_feedLifeCycle_y_17', 'u_phonePrice_15', 'u_refreshTimes_x_8', 'u_refreshTimes_y_8', 'u_userId_137787', 'user_id_137787', '[EOS]']\n",
      "Token IDs: [78373, 1, 3, 7, 7, 8, 7, 61561, 58042, 61378, 9316, 14829, 69811, 10504, 7477, 60377, 70357, 54466, 58876, 67749, 38471, 48134, 46103, 71897, 43375, 58883, 69525, 33585, 31955, 26644, 764, 11099, 19264, 49854, 60014, 22627, 49011, 7314, 18250, 16553, 71234, 67420, 50804, 59023, 28025, 52972, 37407, 16692, 18349, 28560, 43752, 61595, 63524, 52792, 38290, 63964, 32992, 73399, 57183, 61114, 36225, 68614, 38772, 45685, 2328, 71480, 1804, 58811, 10855, 21047, 50165, 72099, 6417, 38474, 27124, 8064, 30576, 46239, 66315, 2196, 14508, 65632, 23803, 68497, 78211, 57769, 31995, 50099, 61646, 55471, 60808, 34487, 78223, 67710, 8017, 21699, 24831, 33070, 55600, 27325, 8664, 78374]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/cotune/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Step 1: Define special tokens\n",
    "BOS_TOKEN = '[BOS]'\n",
    "EOS_TOKEN = '[EOS]'\n",
    "PAD_TOKEN = '[PAD]'\n",
    "UNK_TOKEN = '[UNK]'\n",
    "\n",
    "# Step 2: Update your token vocabulary to include special tokens (if not already present)\n",
    "token_vocab.extend([BOS_TOKEN, EOS_TOKEN, PAD_TOKEN, UNK_TOKEN])\n",
    "\n",
    "# Step 3: Define a regular expression for detecting userIDs.\n",
    "number_regex = re.compile(r'\\d+')\n",
    "\n",
    "# Step 4: Reinitialize the custom tokenizer to handle digit splitting\n",
    "class CustomTokenizer(PreTrainedTokenizer):\n",
    "    def __init__(self, vocab, **kwargs):\n",
    "        self.vocab = {token: i for i, token in enumerate(vocab)}\n",
    "        super().__init__(**kwargs)\n",
    "        self.ids_to_tokens = {i: token for token, i in self.vocab.items()}\n",
    "        self.bos_token = BOS_TOKEN\n",
    "        self.eos_token = EOS_TOKEN\n",
    "        self.pad_token = PAD_TOKEN\n",
    "        self.unk_token = UNK_TOKEN\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        tokens = []\n",
    "        # Split text into words\n",
    "        words = text.split()\n",
    "\n",
    "        for word in words:\n",
    "            # If the word is a number, split into individual digits\n",
    "            if number_regex.fullmatch(word):\n",
    "                tokens.extend(list(word))  # Split the number into digits\n",
    "            else:\n",
    "                tokens.append(word)\n",
    "\n",
    "        # Add BOS and EOS tokens\n",
    "        tokens = [self.bos_token] + tokens + [self.eos_token]\n",
    "        return tokens\n",
    "\n",
    "    def _convert_token_to_id(self, token):\n",
    "        return self.vocab.get(token, self.vocab[self.unk_token])\n",
    "\n",
    "    def _convert_id_to_token(self, index):\n",
    "        return self.ids_to_tokens.get(index, self.unk_token)\n",
    "\n",
    "    def get_vocab(self):\n",
    "        return self.vocab\n",
    "\n",
    "# Step 5: Initialize the tokenizer\n",
    "tokenizer = CustomTokenizer(vocab=token_vocab)\n",
    "\n",
    "# Step 6: Test tokenization with BOS, EOS, and number handling\n",
    "text = dataset['test'][0]['text']\n",
    "tokens = tokenizer.tokenize(text)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "print(f\"Tokens with BOS/EOS and number handling: {tokens}\")\n",
    "print(f\"Token IDs: {token_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function <lambda> at 0x7fe0261e6de0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 32984/32984 [00:23<00:00, 1430.44 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8246/8246 [00:05<00:00, 1448.69 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Tokenize the dataset and pad/truncate to a given max sequence length\n",
    "def tokenize_function(examples, max_length):\n",
    "    # Tokenize the text, ensure padding and truncation to max_length, including BOS/EOS tokens\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,        # Truncate sequences longer than max_length\n",
    "        padding=\"max_length\",   # Pad sequences shorter than max_length\n",
    "        max_length=max_length,  # Define the max length\n",
    "        add_special_tokens=True # Add BOS/EOS tokens\n",
    "    )\n",
    "    \n",
    "    # In autoregressive training, the labels are the same as the input_ids\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    \n",
    "    return tokenized\n",
    "\n",
    "# Step 2: Define your max_length \n",
    "# Max columns in dataset = 120\n",
    "max_length = 128 \n",
    "\n",
    "# Step 3: Apply the tokenizer to the dataset, ensuring all examples are padded to max_length\n",
    "tokenized_datasets = dataset.map(lambda x: tokenize_function(x, max_length), batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '282939 ad_click_list_v001_10670 ad_click_list_v001_17693 ad_click_list_v001_27955 ad_click_list_v001_35131 ad_click_list_v001_23285 ad_click_list_v002_1220 ad_click_list_v002_1361 ad_click_list_v002_1518 ad_click_list_v002_1961 ad_click_list_v002_1173 ad_click_list_v003_280 ad_click_list_v003_240 ad_click_list_v003_114 ad_click_list_v003_162 ad_click_list_v003_246 ad_close_list_v001_24107 ad_close_list_v002_1218 ad_close_list_v003_173 hispace_app_tags_47 u_newsCatInterests_216 u_newsCatInterests_0 u_newsCatInterests_169 u_newsCatInterests_171 u_newsCatInterests_168 u_newsCatDislike_0 u_click_ca2_news_86 u_click_ca2_news_169 u_click_ca2_news_171 u_click_ca2_news_168 u_click_ca2_news_78 i_entities_1431cd8e8f17b247cbfb2c67be86a0d8bd246262fe7e99e3c97a91a03ffb31bb i_entities_8c2626d7bc49908761f81d427d4d350ddc5a5904c85c9f8265011d3ea8682d42 i_entities_a6dd053044008fc2e0884d4714caeec969cfbf2488d7a1a2fca4770f6c61f3b2 i_entities_c36449314af7aac8ad712c095dde96c62c684f02b6b2a7ee7307bb47e3c1e579 i_entities_e86f2e151c68d11ec5ec047d3aa135b1508567cc09226ed83299420eb3e0470b u_newsCatInterestsST_x_199 u_newsCatInterestsST_x_62 u_newsCatInterestsST_x_73 u_newsCatInterestsST_x_65 u_newsCatInterestsST_x_41 u_newsCatInterestsST_y_171 u_newsCatInterestsST_y_169 u_newsCatInterestsST_y_199 u_newsCatInterestsST_y_168 u_newsCatInterestsST_y_78 adv_id_16606 adv_prim_id_1401 age_8 app_score_0.0 app_second_class_23 cillabel_-1 city_285 city_rank_2 creat_type_cd_8 device_name_150 device_size_2401 e_ch_19 e_et_202206101119 e_m_183 e_pl_649 e_po_4 e_rn_10 e_section_0 emui_dev_21 gender_2 i_cat_151 i_dislikeTimes_0 i_docId_728c7deca83636b64f8b4ffe016b32862a19eac2 i_dtype_12 i_regionEntity_2859 i_s_sourceId_14cbe3735e6fd61add145586131b3cab9eb4280b i_upTimes_0 inter_type_cd_5 label_-1 log_id_765900 net_type_7 pro_0 pt_d_202206100511 residence_29 series_dev_16 series_group_5 site_id_1 slot_id_35 spread_app_id_246 task_id_35971 u_browserLifeCycle_17 u_browserMode_14 u_feedLifeCycle_x_17 u_feedLifeCycle_y_17 u_phonePrice_10 u_refreshTimes_x_9 u_refreshTimes_y_9 u_userId_282939 user_id_282939',\n",
       " 'input_ids': [78373,\n",
       "  2,\n",
       "  8,\n",
       "  2,\n",
       "  9,\n",
       "  3,\n",
       "  9,\n",
       "  76558,\n",
       "  47601,\n",
       "  12232,\n",
       "  74591,\n",
       "  70011,\n",
       "  55085,\n",
       "  23687,\n",
       "  74724,\n",
       "  58674,\n",
       "  12356,\n",
       "  45463,\n",
       "  58876,\n",
       "  18041,\n",
       "  54466,\n",
       "  38471,\n",
       "  46103,\n",
       "  71897,\n",
       "  43375,\n",
       "  45994,\n",
       "  64540,\n",
       "  73510,\n",
       "  15375,\n",
       "  68292,\n",
       "  64825,\n",
       "  11099,\n",
       "  73192,\n",
       "  69053,\n",
       "  39491,\n",
       "  49854,\n",
       "  74742,\n",
       "  30029,\n",
       "  14738,\n",
       "  30025,\n",
       "  60909,\n",
       "  22828,\n",
       "  24790,\n",
       "  33511,\n",
       "  61682,\n",
       "  59662,\n",
       "  20985,\n",
       "  34359,\n",
       "  59767,\n",
       "  60477,\n",
       "  61595,\n",
       "  14960,\n",
       "  77328,\n",
       "  9510,\n",
       "  34898,\n",
       "  73176,\n",
       "  16334,\n",
       "  73399,\n",
       "  43232,\n",
       "  61114,\n",
       "  36225,\n",
       "  54755,\n",
       "  18794,\n",
       "  45685,\n",
       "  31981,\n",
       "  13633,\n",
       "  10235,\n",
       "  17600,\n",
       "  34892,\n",
       "  21047,\n",
       "  22017,\n",
       "  72099,\n",
       "  65636,\n",
       "  46140,\n",
       "  8398,\n",
       "  8064,\n",
       "  24275,\n",
       "  57644,\n",
       "  72629,\n",
       "  57170,\n",
       "  14508,\n",
       "  23742,\n",
       "  23803,\n",
       "  68497,\n",
       "  45048,\n",
       "  39536,\n",
       "  9043,\n",
       "  78138,\n",
       "  61646,\n",
       "  16778,\n",
       "  44940,\n",
       "  7296,\n",
       "  78223,\n",
       "  67710,\n",
       "  8017,\n",
       "  21699,\n",
       "  57653,\n",
       "  76556,\n",
       "  71247,\n",
       "  73969,\n",
       "  39363,\n",
       "  78374,\n",
       "  78375,\n",
       "  78375,\n",
       "  78375,\n",
       "  78375,\n",
       "  78375,\n",
       "  78375,\n",
       "  78375,\n",
       "  78375,\n",
       "  78375,\n",
       "  78375,\n",
       "  78375,\n",
       "  78375,\n",
       "  78375,\n",
       "  78375,\n",
       "  78375,\n",
       "  78375,\n",
       "  78375,\n",
       "  78375,\n",
       "  78375,\n",
       "  78375,\n",
       "  78375,\n",
       "  78375,\n",
       "  78375,\n",
       "  78375,\n",
       "  78375,\n",
       "  78375],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'labels': [78373,\n",
       "  2,\n",
       "  8,\n",
       "  2,\n",
       "  9,\n",
       "  3,\n",
       "  9,\n",
       "  76558,\n",
       "  47601,\n",
       "  12232,\n",
       "  74591,\n",
       "  70011,\n",
       "  55085,\n",
       "  23687,\n",
       "  74724,\n",
       "  58674,\n",
       "  12356,\n",
       "  45463,\n",
       "  58876,\n",
       "  18041,\n",
       "  54466,\n",
       "  38471,\n",
       "  46103,\n",
       "  71897,\n",
       "  43375,\n",
       "  45994,\n",
       "  64540,\n",
       "  73510,\n",
       "  15375,\n",
       "  68292,\n",
       "  64825,\n",
       "  11099,\n",
       "  73192,\n",
       "  69053,\n",
       "  39491,\n",
       "  49854,\n",
       "  74742,\n",
       "  30029,\n",
       "  14738,\n",
       "  30025,\n",
       "  60909,\n",
       "  22828,\n",
       "  24790,\n",
       "  33511,\n",
       "  61682,\n",
       "  59662,\n",
       "  20985,\n",
       "  34359,\n",
       "  59767,\n",
       "  60477,\n",
       "  61595,\n",
       "  14960,\n",
       "  77328,\n",
       "  9510,\n",
       "  34898,\n",
       "  73176,\n",
       "  16334,\n",
       "  73399,\n",
       "  43232,\n",
       "  61114,\n",
       "  36225,\n",
       "  54755,\n",
       "  18794,\n",
       "  45685,\n",
       "  31981,\n",
       "  13633,\n",
       "  10235,\n",
       "  17600,\n",
       "  34892,\n",
       "  21047,\n",
       "  22017,\n",
       "  72099,\n",
       "  65636,\n",
       "  46140,\n",
       "  8398,\n",
       "  8064,\n",
       "  24275,\n",
       "  57644,\n",
       "  72629,\n",
       "  57170,\n",
       "  14508,\n",
       "  23742,\n",
       "  23803,\n",
       "  68497,\n",
       "  45048,\n",
       "  39536,\n",
       "  9043,\n",
       "  78138,\n",
       "  61646,\n",
       "  16778,\n",
       "  44940,\n",
       "  7296,\n",
       "  78223,\n",
       "  67710,\n",
       "  8017,\n",
       "  21699,\n",
       "  57653,\n",
       "  76556,\n",
       "  71247,\n",
       "  73969,\n",
       "  39363,\n",
       "  78374,\n",
       "  78375,\n",
       "  78375,\n",
       "  78375,\n",
       "  78375,\n",
       "  78375,\n",
       "  78375,\n",
       "  78375,\n",
       "  78375,\n",
       "  78375,\n",
       "  78375,\n",
       "  78375,\n",
       "  78375,\n",
       "  78375,\n",
       "  78375,\n",
       "  78375,\n",
       "  78375,\n",
       "  78375,\n",
       "  78375,\n",
       "  78375,\n",
       "  78375,\n",
       "  78375,\n",
       "  78375,\n",
       "  78375,\n",
       "  78375,\n",
       "  78375,\n",
       "  78375]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/cotune/lib/python3.12/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='516' max='516' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [516/516 01:47, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Step 2: Define the GPT-2 model architecture for a distill model\n",
    "# You can configure the distillation process by reducing the number of layers, heads, etc.\n",
    "config = GPT2Config(\n",
    "    vocab_size=len(tokenizer.get_vocab()),\n",
    "    n_embd=256,  # Smaller embedding size for distillation\n",
    "    n_layer=6,   # Fewer layers than standard GPT-2\n",
    "    n_head=4,    # Fewer attention heads\n",
    "    n_positions=max_length,  # Position embeddings\n",
    ")\n",
    "\n",
    "# Initialize a new GPT-2 model with the custom configuration\n",
    "model = GPT2LMHeadModel(config)\n",
    "\n",
    "# Step 3: Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"checkpoints/gpt2-distilled\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=64,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=500,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=1000,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "# Step 4: Define the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],  # Optional, if validation set is available\n",
    ")\n",
    "\n",
    "# Step 5: Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Step 6: Save the model\n",
    "trainer.save_model(\"checkpoints/gpt2-distilled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Step 1: Function to generate sentences based on first half of input_ids\n",
    "def complete_sentences(model, tokenizer, tokenized_dataset, max_length):\n",
    "    completed_sentences = []\n",
    "    \n",
    "    # Ensure model is in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    for example in tqdm(tokenized_dataset):\n",
    "        input_ids = example['input_ids']\n",
    "        \n",
    "        # Step 2: Take the first half of the input_ids as the prompt\n",
    "        half_length = len(input_ids) // 2\n",
    "        prompt_ids = input_ids[:half_length]\n",
    "        \n",
    "        # Step 3: Use the model to generate the complete sentence\n",
    "        input_ids_tensor = torch.tensor([prompt_ids]).to(model.device)  # Add batch dimension\n",
    "        generated_ids = model.generate(\n",
    "            input_ids=input_ids_tensor,\n",
    "            max_length=max_length,  # Generate up to the max length\n",
    "            pad_token_id=tokenizer.pad_token_id,  # Ensure proper padding handling\n",
    "            eos_token_id=tokenizer.eos_token_id  # Stop at EOS token\n",
    "        )\n",
    "        \n",
    "        # Step 4: Convert generated token IDs back to text\n",
    "        generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "        completed_sentences.append(generated_text)\n",
    "    \n",
    "    return completed_sentences\n",
    "\n",
    "# Generate completed sentences as text\n",
    "completed_sentences = complete_sentences(model, tokenizer, tokenized_datasets['test'], max_length)\n",
    "\n",
    "# Completed sentences will now be a list of text where each element is a completed sentence\n",
    "print(completed_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137787 ad_click_list_v001_28621 ad_click_list_v001_21424 ad_click_list_v001_24055 ad_click_list_v001_17305 ad_click_list_v001_31470 ad_click_list_v002_1203 ad_click_list_v002_1172 ad_click_list_v002_1112 ad_click_list_v002_1775 ad_click_list_v002_1041 ad_click_list_v003_162 ad_click_list_v003_240 ad_click_list_v003_343 ad_click_list_v003_246 ad_click_list_v003_312 ad_close_list_v001_24107 ad_close_list_v002_1218 ad_close_list_v003_173 hispace_app_tags_43 u_newsCatInterests_140 u_newsCatInterests_112 u_newsCatInterests_16 u_newsCatInterests_176 u_newsCatInterests_207 u_newsCatDislike_0 u_click_ca2_news_112 u_click_ca2_news_168 u_click_ca2_news_140 u_click_ca2_news_207 u_click_ca2_news_15 i_entities_5b212d9859cc262a2d9f4731b8e1890be315e4d27e4d4602bdc993ec955cdfac i_entities_8e1358ee2230f9112e0464bba2cc119224a6849fd6477d6a316eb358e0bbff14 i_entities_064d7e92c0b22a54f65e6193db3f201ed58258a1f17bed583f1359423fcf7331 i_entities_c81ec0fd7307cf51be43e50261cf60c724d1972d358be6ddb8a1f1cb191adf98 i_entities_9a7ddbfecfcdb8eb62f864021b755cff68ff16d8e9d644aa3b57f876fd7b4538 u_newsCatInterestsST_x_140 u_newsCatInterestsST_x_207 u_newsCatInterestsST_x_176 u_newsCatInterestsST_x_157 u_newsCatInterestsST_x_52 u_newsCatInterestsST_y_140 u_newsCatInterestsST_y_157 u_newsCatInterestsST_y_15 u_newsCatInterestsST_y_50 u_newsCatInterestsST_y_168 adv_id_17944 adv_prim_id_1779 age_3 app_score_10.0 app_second_class_18 cillabel_-1 city_103 city_rank_2 creat_type_cd_8 device_name_211 device_size_2397 e_ch_19 e_et_202206102219 e_m_1363 e_pl_1728 e_po_6 e_rn_1 e_section_1 emui_dev_13 gender_2 i_cat_98 i_dislikeTimes_0 i_docId_be238d80eb08e026444846a14e307e2eda256887 i_dtype_11 i_regionEntity_0 i_s_sourceId_645e57afbbafce5159ab7fe956837c40103f33c9 i_upTimes_7 inter_type_cd_4 label_-1 log_id_83023 net_type_7 pro_0 pt_d_202206101016 residence_20 series_dev_30 series_group_3 site_id_1 slot_id_50 spread_app_id_168 task_id_20964 u_browserLifeCycle_17 u_browserMode_14 u_feedLifeCycle_x_17 u_feedLifeCycle_y_17 u_phonePrice_15 u_refreshTimes_x_9 u_refreshTimes_y_9 u_userId_111341 user_id_147274\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def combine_digits(sentence):\n",
    "    # Use regex to find sequences of single digits and combine them into a single number\n",
    "    processed_sentence = re.sub(r'(?<=\\b)(\\d\\s)+\\d(?=\\b)', lambda x: ''.join(x.group(0).split()), sentence)\n",
    "    return processed_sentence\n",
    "\n",
    "def process_completed_sentences(completed_sentences):\n",
    "    # Apply the combine_digits function to each sentence\n",
    "    processed_sentences = [combine_digits(sentence) for sentence in completed_sentences]\n",
    "    return processed_sentences\n",
    "\n",
    "processed_sentences = process_completed_sentences(completed_sentences)\n",
    "print(processed_sentences[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed sentences have been saved to conditional_generation.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Function to save completed sentences to a file\n",
    "def save_sentences_to_file(completed_sentences, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        for sentence in completed_sentences:\n",
    "            f.write(sentence + '\\n')  # Write each sentence on a new line\n",
    "\n",
    "syn_dir = \"synth_data\"\n",
    "os.makedirs(syn_dir, exist_ok=True)\n",
    "file_path = 'conditional_generation.txt'  # Specify the file path\n",
    "save_sentences_to_file(processed_sentences, os.path.join(syn_dir,file_path))\n",
    "\n",
    "print(f\"Completed sentences have been saved to {file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
